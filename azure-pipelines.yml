trigger: none

variables:
  serviceConnection: 'terraform-access'
  GO_VERSION: '1.24.5'
  TFSEC_VERSION: 'v1.28.14'
  TFLINT_VERSION: 'v0.59.1'
  doDestroy: 'true'  # set to 'true' to enable destroy stage
  RG_NAME_PREFIX: 'ascension-up'
  location: 'northeurope'


stages:
# =====================================
# DEV STAGE (build -> infra -> apps)  #
# =====================================
- stage: dev
  displayName: 'DEV'
  variables: { env: 'dev' }
  jobs:
  # ---- dev_infra ----
  - job: dev_infra
    displayName: 'Provision infra (dev)'
    pool: { name: 'ubuntuvm' }
    steps:
      - checkout: self
        clean: true

      - task: AzureCLI@2
        displayName: 'Login + tools + plan/apply (dev)'
        inputs:
          azureSubscription: '$(serviceConnection)'
          scriptType: bash
          scriptLocation: inlineScript
          inlineScript: |
            set -euo pipefail
            az account show

            BIN_DIR="$(Agent.TempDirectory)/bin"; mkdir -p "$BIN_DIR"; export PATH="$BIN_DIR:$PATH"
            # (install go/tfsec/tflint as before) ...
            tflint --init
            tflint --recursive
            tfsec "$(Build.SourcesDirectory)/terraform" || true

            TF_DIR="$(Build.SourcesDirectory)/terraform"
            terraform -chdir="$TF_DIR" init -input=false

            # --- Workspace: create only if missing ---
            ensure_ws() {
              local dir="$1" ws="$2"
              if terraform -chdir="$dir" workspace list | sed 's/*//;s/ //g' | grep -Fxq "$ws"; then
                  terraform -chdir="$dir" workspace select "$ws"
              else
                terraform -chdir="$dir" workspace new "$ws"
              fi
            }
            ensure_ws "$TF_DIR" "$(env)"

            # --- Import-if-exists guard for Resource Group ---
            SUB_ID=$(az account show --query id -o tsv)
            RG_NAME="$(RG_NAME_PREFIX)-$(env)-rg"
            if az group exists -n "$RG_NAME"; then
              if ! terraform -chdir="$TF_DIR" state show azurerm_resource_group.ascension_test_rg >/dev/null 2>&1; then
                terraform -chdir="$TF_DIR" import azurerm_resource_group.ascension_test_rg "/subscriptions/$SUB_ID/resourceGroups/$RG_NAME" || true
              fi
            fi

            # --- Resolve pipeline principal (you already pass it into TF as a var) ---
            # If you prefer a fixed object id, set PIPELINE_PRINCIPAL_OBJECT_ID directly.
            SC_CLIENT_ID="331a5bcb-b95d-4529-a528-c858e28d9a89"
            PIPELINE_PRINCIPAL_OBJECT_ID=$(az ad sp show --id "$SC_CLIENT_ID" --query id -o tsv)

            # ===== Import-if-exists guards for KV role assignments (BEFORE plan) =====
            TF_ADDR_SELF="module.app_service.azurerm_role_assignment.kv_secrets_officer_self"
            TF_ADDR_PIPE="module.app_service.azurerm_role_assignment.kv_secrets_officer_pipeline[0]"

            # Resolve KV id by naming pattern: kv-<env>-ascension-<random>
            KV_ID=$(az keyvault list -g "$RG_NAME" \
              --query "[?starts_with(name, 'kv-$(env)-ascension-')].id | [0]" -o tsv || true)

            if [ -n "$KV_ID" ]; then
              ROLE_NAME="Key Vault Secrets Officer"

            # Self principal (current context). On pipelines this is the service principal.
            # Try signed-in user (may be empty in SP context), then fall back to the SC client id object.
            SELF_OBJECT_ID=$(az ad signed-in-user show --query id -o tsv 2>/dev/null || true)
            if [ -z "${SELF_OBJECT_ID:-}" ]; then
              SELF_OBJECT_ID=$(az ad sp show --id "$SC_CLIENT_ID" --query id -o tsv 2>/dev/null || true)
            fi

            if [ -n "${SELF_OBJECT_ID:-}" ]; then
              EXIST_SELF=$(az role assignment list \
                --assignee-object-id "$SELF_OBJECT_ID" \
                --scope "$KV_ID" \
                --role "$ROLE_NAME" \
                --query "[0].id" -o tsv || true)
              if [ -n "$EXIST_SELF" ]; then
                echo "Importing existing KV role assignment (self): $EXIST_SELF"
                terraform -chdir="$TF_DIR" import "$TF_ADDR_SELF" "$EXIST_SELF" || true
              fi
            fi

            # Pipeline principal (if provided)
            if [ -n "${PIPELINE_PRINCIPAL_OBJECT_ID:-}" ] && [ "${PIPELINE_PRINCIPAL_OBJECT_ID}" != "null" ]; then
              EXIST_PIPE=$(az role assignment list \
                --assignee-object-id "$PIPELINE_PRINCIPAL_OBJECT_ID" \
                --scope "$KV_ID" \
                --role "$ROLE_NAME" \
                --query "[0].id" -o tsv || true)
              if [ -n "$EXIST_PIPE" ]; then
                echo "Importing existing KV role assignment (pipeline): $EXIST_PIPE"
                terraform -chdir="$TF_DIR" import "$TF_ADDR_PIPE" "$EXIST_PIPE" || true
              fi
            fi
            else
              echo "KV not found yet in RG $RG_NAME (name carries a random suffix). Skipping role import guard."
            fi
            # =======================================================================

            # --- Plan / Apply ---
            terraform -chdir="$TF_DIR" plan -no-color -input=false \
              -var="workflow=$(env)" \
              -var="pipeline_principal_id=$PIPELINE_PRINCIPAL_OBJECT_ID" \
              -var="frontend_image_name=frontend-$(env)" \
              -var="frontend_image_tag=${BUILD_BUILDID}" \
              -out="tfplan.$(env).out"

            if [ "$(Build.SourceBranch)" = "refs/heads/main" ]; then
              terraform -chdir="$TF_DIR" apply -auto-approve -no-color "tfplan.$(env).out"
              terraform -chdir="$TF_DIR" output -json > "$TF_DIR/tf-outputs.$(env).json"
            else
              echo "Not main branch, skipping apply."
            fi

      - publish: $(Build.SourcesDirectory)/terraform/tf-outputs.$(env).json
        artifact: tf-outputs-$(env)
        condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
        displayName: 'Publish tf-outputs'

  # ---- dev_func_local_test ----
  - job: dev_func_local_test
    displayName: 'Azure Function: venv + func start + curl + pytest'
    pool: { name: 'ubuntuvm' }
    steps:
      - checkout: self
        clean: true

      # Install Node for Core Tools (works on self-hosted agents too)
      - task: NodeTool@0
        displayName: 'Use Node 18.x'
        inputs:
          versionSpec: '18.x'

      - script: npm i -g azure-functions-core-tools@4 --unsafe-perm true
        displayName: 'Install Azure Functions Core Tools v4'

      - bash: |
          set -euo pipefail
          cd "$(Build.SourcesDirectory)/api-function"

          # venv + deps
          python3 -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r requirements.txt

          # local settings
          cp -f local.settings.example.json local.settings.json

          # start Functions host in background
          LOG_FILE="$(Build.SourcesDirectory)/.func.log"
          nohup func start --python > "$LOG_FILE" 2>&1 & echo $! > .func.pid

          echo "Waiting for local endpoint /api/products ..."
          for i in {1..60}; do
            if curl -fsS "http://localhost:7071/api/products" > /dev/null; then
              echo "Endpoint is up."
              break
            fi
            sleep 2
          done

          echo "Smoke test response:"
          curl -v "http://localhost:7071/api/products" || true

          # run tests
          pytest -q
        displayName: 'Create venv, start func, smoke-test endpoint, run pytest'

      - bash: |
          set -euo pipefail
          cd "$(Build.SourcesDirectory)/api-function"
          if [ -f ".func.pid" ]; then
            echo "Stopping Functions host (PID $(cat .func.pid)) ..."
            kill "$(cat .func.pid)" || true
            pkill -f "func start" || true
            sleep 2
          fi
          echo "Last 200 lines of Functions host log:"
          tail -n 200 "$(Build.SourcesDirectory)/.func.log" || true
        displayName: 'Stop func & show logs'
        condition: always()

  # ---- build_dev ----
  - job: build_dev
    displayName: 'Build frontend for React with App Service & package function (dev)'
    dependsOn: 
      - dev_infra
      - dev_func_local_test
    pool: { name: 'ubuntuvm' }
    steps:
      - checkout: self
        clean: true

      # ACR login server from Terraform outputs is needed to build & push container
      - download: current
        artifact: tf-outputs-$(env)

      - task: AzureCLI@2
        displayName: 'Login + Build & Push Frontend Container to ACR'
        inputs:
          azureSubscription: '$(serviceConnection)'
          scriptType: bash
          scriptLocation: inlineScript
          inlineScript: |
            set -euo pipefail

            TF_OUT="$(Pipeline.Workspace)/tf-outputs-$(env)/tf-outputs.$(env).json"
            test -f "$TF_OUT" || { echo "Missing TF outputs at $TF_OUT"; exit 1; }

            ACR_LOGIN_SERVER=$(python3 -c "import json;print(json.load(open('$TF_OUT'))['acr_login_server']['value'])")
            test -n "$ACR_LOGIN_SERVER" || { echo "acr_login_server missing in TF outputs"; exit 1; }
            ACR_NAME="${ACR_LOGIN_SERVER%%.*}"

            echo "Using ACR: $ACR_NAME ($ACR_LOGIN_SERVER)"

            # Ensure Docker is available on your self-hosted agent
            if ! command -v docker >/dev/null 2>&1; then
              echo "Docker not found on agent. Please install & start Docker on 'ubuntuvm'." >&2
              exit 1
            fi

            # Log in to ACR (OIDC-backed service connection is already logged in to Azure)
            az acr login -n "$ACR_NAME"

            # Build from your existing repo structure: frontend/ (expects Dockerfile + nginx.conf there)
            FRONTEND_DIR="$(Build.SourcesDirectory)/frontend"
            test -f "$FRONTEND_DIR/Dockerfile" || { echo "frontend/Dockerfile not found"; exit 1; }
            test -f "$FRONTEND_DIR/nginx.conf" || { echo "frontend/nginx.conf not found"; exit 1; }

            IMG_NAME="frontend-$(env)"
            IMG_TAG="$(Build.BuildId)"
            IMAGE_URI="${ACR_LOGIN_SERVER}/${IMG_NAME}:${IMG_TAG}"

            echo "Building ${IMAGE_URI} from ${FRONTEND_DIR}"
            docker build -t "$IMAGE_URI" "$FRONTEND_DIR"
            docker push "$IMAGE_URI"

            # Save image metadata for deploy job
            META_DIR="$(Build.ArtifactStagingDirectory)/frontend-image"
            mkdir -p "$META_DIR"
            cat > "$META_DIR/image.json" <<EOF
              {
                "acr_login_server": "$ACR_LOGIN_SERVER",
                "image_name": "$IMG_NAME",
                "image_tag": "$IMG_TAG",
              "image_uri": "$IMAGE_URI"
            }
            EOF

      - publish: $(Build.ArtifactStagingDirectory)/frontend-image
        artifact: frontend-image

      # Package Python Azure Function (zip) from your repo (api-function/)
      - bash: |
          set -euo pipefail
          FUNC_SRC="$(Build.SourcesDirectory)/api-function"
          OUT_DIR="$(Build.ArtifactStagingDirectory)/function"
          mkdir -p "$OUT_DIR"

          # Ensure expected files exist
          test -f "$FUNC_SRC/host.json" || { echo "api-function/host.json missing"; exit 1; }

          # vendor deps into .python_packages so theyâ€™re part of the zip
          python3 -m pip install --upgrade pip
          python3 -m pip install -r "$FUNC_SRC/requirements.txt" --target "$FUNC_SRC/.python_packages/lib/site-packages"

          # Zip (exclude venv/.git/__pycache__)
          cd "$FUNC_SRC"
          zip -r "$OUT_DIR/function.zip" . \
            -x ".*" -x "__pycache__/*" -x ".venv/*" -x "venv/*" >/dev/null
          echo "Created $OUT_DIR/function.zip"
        displayName: 'Package Python Azure Function (zip)'

      - publish: $(Build.ArtifactStagingDirectory)/azure-function-python
        artifact: azure-function-python


  # ---- dev_apps ----
  - job: deploy_apps
    displayName: 'Deploy apps (dev)'
    dependsOn:
      - build_dev
    condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
    pool: { name: 'ubuntuvm' }
    steps:
      - download: current
        artifact: frontend-image
      - download: current
        artifact: function
      - download: current
        artifact: tf-outputs-$(env)

      - task: AzureCLI@2
        displayName: 'Point Web App to ACR image (Linux) + Deploy Function Zip'
        inputs:
          azureSubscription: '$(serviceConnection)'
          scriptType: bash
          scriptLocation: inlineScript
          inlineScript: |
            set -euo pipefail

            TF_OUT="$(Pipeline.Workspace)/tf-outputs-$(env)/tf-outputs.$(env).json"
            IMG_META="$(Pipeline.Workspace)/frontend-image/image.json"
            test -f "$TF_OUT"   || { echo "Missing TF outputs $TF_OUT"; exit 1; }
            test -f "$IMG_META" || { echo "Missing image metadata $IMG_META"; exit 1; }

            RG=$(python3 -c "import json;print(json.load(open('$TF_OUT'))['resource_group_name']['value'])")
            WEB=$(python3 -c "import json;print(json.load(open('$TF_OUT'))['react_web_name']['value'])")
            FN=$(python3 -c "import json;print(json.load(open('$TF_OUT'))['function_name']['value'])")
            ACR_LOGIN_SERVER=$(python3 -c "import json;print(json.load(open('$IMG_META'))['acr_login_server'])")
            IMAGE_URI=$(python3 -c "import json;print(json.load(open('$IMG_META'))['image_uri'])")

            echo "RG=$RG WEB=$WEB FN=$FN"
            echo "IMAGE_URI=$IMAGE_URI"

            # Ensure target Web App is Linux
            KIND=$(az webapp show -g "$RG" -n "$WEB" --query "kind" -o tsv)
            RESERVED=$(az webapp show -g "$RG" -n "$WEB" --query "reserved" -o tsv)
            if [[ "${RESERVED}" != "true" && "${KIND}" != *"linux"* ]]; then
              echo "ERROR: Web App '$WEB' is not Linux."; exit 1
            fi

            # Configure Web App to use ACR image (managed identity pull)
            az webapp config container set \
              -g "$RG" -n "$WEB" \
              --docker-custom-image-name "$IMAGE_URI" \
              --docker-registry-server-url "https://${ACR_LOGIN_SERVER}" || true

            az webapp config set -g "$RG" -n "$WEB" --acr-use-managed-identity
            az webapp config appsettings set -g "$RG" -n "$WEB" --settings \
              WEBSITES_ENABLE_APP_SERVICE_STORAGE=false \
              WEBSITES_PORT=80
            az webapp restart -g "$RG" -n "$WEB"

            # Deploy Function zip
            FUNCTION_ZIP="$(Pipeline.Workspace)/function/function.zip"
            test -f "$FUNCTION_ZIP" || { echo "Function zip not found at $FUNCTION_ZIP"; exit 1; }
            az functionapp config appsettings set -g "$RG" -n "$FN" --settings SCM_DO_BUILD_DURING_DEPLOYMENT=true
            az functionapp deployment source config-zip -g "$RG" -n "$FN" --src "$FUNCTION_ZIP"

            # Best-effort smoke check
            curl -fsS "https://${FN}.azurewebsites.net/api/products" || true


# ===== Optional destroy (linear after prod) =====
- stage: destroy
  displayName: 'Terraform destroy (guarded)'
  #dependsOn: prod
  dependsOn: dev
  condition: and(succeeded(), eq(variables['doDestroy'], 'true'))
  jobs:
  - job: destroy_all
    pool: { name: 'ubuntuvm' }
    strategy:
      matrix:
        dev:  { env: dev }
        #test: { env: test }
        #prod: { env: prod }
    steps:
      - checkout: self
        clean: true
      - task: AzureCLI@2
        displayName: "Destroy $(env)"
        inputs:
          azureSubscription: '$(serviceConnection)'
          scriptType: bash
          scriptLocation: inlineScript
          inlineScript: |
            set -euo pipefail
            terraform -chdir="$(Build.SourcesDirectory)/terraform" init -input=false
            terraform -chdir="$(Build.SourcesDirectory)/terraform" workspace select -or-create "$(env)"
            terraform -chdir="$(Build.SourcesDirectory)/terraform" destroy -no-color -auto-approve -var="workflow=$(env)"
